{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BIBLIOTECAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNÇÕES**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UTILIDADES**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CRIANDO DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "  return pd.DataFrame(columns = ['Data', 'endereco'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LISTANDO DIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_days_since(since: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a list of dates with all days since a given date.\n",
    "    The start_date parameter must be a string date with the format: '%Y-%m-%d'\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.date_range(start=since, end=datetime.datetime.today().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WEB SCRAPING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REQUEST G1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request_page(q: str, date: str, source: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Solicita a URL de pesquisa da fonte especificada (por exemplo, 'g1' ou 'valor econômico') \n",
    "    e retorna o objeto de solicitação se o status_code for igual a 200.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"\"\n",
    "\n",
    "    if source.lower() == \"g1\":\n",
    "        url = f'https://g1.globo.com/busca/?q={q}&page=1&order=relevant&species=notícias&from={date}T00%3A00%3A00-0300&to={date}T23%3A59%3A59-0300'\n",
    "\n",
    "    elif source.lower() == \"valor\":\n",
    "        url = f'https://valor.globo.com/busca/?q={q}&page=1&order=relevant&species=notícias&from={date}T00%3A00%3A00-0300&to={date}T23%3A59%3A59-0300'\n",
    "    \n",
    "    else:\n",
    "        print(f\"Source '{source}' não reconhecido.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        req = requests.get(url, timeout= 60.0)\n",
    "\n",
    "        if req.status_code == 200:\n",
    "            return req\n",
    "\n",
    "    except Exception as e:\n",
    "        err_name = type(e).__name__\n",
    "        print(f'Requisition error for URL {url}: {err_name}')\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CARREGANDO PÁGINA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(response):\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  return soup.select('.widget--info__text-container a')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BUSCANDO LINK NOTÍCIAS G1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_g1(start: str, source: str):\n",
    "\n",
    "  dataset = create_dataset()\n",
    "  days = all_days_since(start)\n",
    "\n",
    "  for i, day in enumerate(days):\n",
    "    dataset.to_excel('dataset_agro bkp.xlsx', encoding='latin-1', index=False) if i % 7 == 0 else None\n",
    "    print(day)\n",
    "\n",
    "    try:\n",
    "      response = get_request_page(q = 'Agronegócio', date = str(day.date()), source = source)\n",
    "      page = get_page(response = response)\n",
    "\n",
    "      for http in page:\n",
    "        \n",
    "        try:\n",
    "          http = http.get('href')\n",
    "          dataset.loc[len(dataset)] = [day,  http]\n",
    "\n",
    "        except Exception as e1:\n",
    "          err_name = type(e1).__name__\n",
    "          print(f'Nãoo achou o link {http}: {err_name}')\n",
    "          continue\n",
    "\n",
    "    except Exception as e2:\n",
    "        err_name = type(e2).__name__\n",
    "        print(f'Requisition error for URL {http}: {err_name}')\n",
    "    \n",
    "  dataset['endereco'] = \"https:\" + dataset['endereco']\n",
    "  dataset.to_excel('dataset_agro.xlsx')\n",
    "    \n",
    "  return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BUSCANDO TEXTO DAS NOTICICAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_http_text(dataset):\n",
    "  textos_completos = []\n",
    "  titulos = []\n",
    "\n",
    "  https = dataset['endereco'].to_list()\n",
    "  restantes = 0\n",
    "  \n",
    "  for i, http in enumerate(https):\n",
    "    print(f\"Restam {len(https)-restantes} consultas\")\n",
    "    sessao = requests.Session()\n",
    "    \n",
    "    try:\n",
    "      url = f'{http}'\n",
    "      req = sessao.get(url, timeout= 15.0)\n",
    "      soup = BeautifulSoup(req.text, 'html.parser')\n",
    "      body_element = soup.body\n",
    "      paragrafos = [p.get_text(separator='\\n', strip=True) for p in body_element.find_all('p')]\n",
    "      texto_completo = '\\n'.join(paragrafos)\n",
    "\n",
    "      textos_completos.append(texto_completo)\n",
    "      pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "      restantes += 1\n",
    "      sessao.close()\n",
    "\n",
    "    except TimeoutError as e1:\n",
    "       texto_completo = \"N/D\"\n",
    "       textos_completos.append(texto_completo)\n",
    "       pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "       print(f\"Erro {e1}, aguardando {time.sleep(600)} segundos\")\n",
    "       sessao.close()\n",
    "       continue\n",
    "  \n",
    "    #EM CASO DE REDIRECIONAMENTO DE LINK\n",
    "    except AttributeError as e2:\n",
    "        \n",
    "        try:\n",
    "            script_redirecionamento = soup.find('script', string=re.compile(r'window\\.location\\.replace\\(\"(.+)\"\\)'))\n",
    "            match = re.search(r'window\\.location\\.replace\\(\"(.+)\"\\)', str(script_redirecionamento))\n",
    "            url_redirecionada = match.group(1)\n",
    "\n",
    "            print(f'{e2} redirecionado para {url_redirecionada}')\n",
    "            \n",
    "            req = requests.get(url_redirecionada, timeout= 300.0)\n",
    "            soup = BeautifulSoup(req.text, 'html.parser')\n",
    "            body_element = soup.body\n",
    "\n",
    "            paragrafos = [p.get_text(separator='\\n', strip=True) for p in body_element.find_all('p')]\n",
    "            texto_completo = '\\n'.join(paragrafos)\n",
    "            \n",
    "            textos_completos.append(texto_completo)\n",
    "            pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "            restantes += 1\n",
    "            sessao.close()\n",
    "\n",
    "        except TimeoutError as e3:\n",
    "            texto_completo = \"N/D\"\n",
    "            textos_completos.append(texto_completo)\n",
    "            pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "            sessao.close()\n",
    "            print(f\"Erro {e3}, aguardando {time.sleep(600)} segundos\")\n",
    "            continue\n",
    "            \n",
    "        except Exception as e4:\n",
    "            texto_completo = \"N/D\"\n",
    "            textos_completos.append(texto_completo)\n",
    "            pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "            print(f\"Erro ao obter URL redirecionada {url_redirecionada}: {e4}\")\n",
    "            time.sleep(600)\n",
    "            restantes +=1\n",
    "            sessao.close()\n",
    "            continue  # Continua para a próxima iteração em caso de erro na URL redirecionada \n",
    "\n",
    "    except Exception as e:\n",
    "      texto_completo = \"N/D\"\n",
    "      textos_completos.append(texto_completo)\n",
    "      pd.DataFrame(textos_completos).to_excel('textos_completos_agro.xlsx', encoding='latin-1', index=False) if i % 100 == 0 else None\n",
    "      print(f\"Erro ao obter URL redirecionada: {e}\")\n",
    "      time.sleep(600)\n",
    "      restantes +=1\n",
    "      sessao.close()\n",
    "      continue\n",
    "\n",
    "  dataset['Texto'] = textos_completos\n",
    "  dataset.to_excel('textos_completos_agro.xlsx', index = False)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRATAMENTO DOS DADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO NOTÍCIAS DUPLICADAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_news(dataset):\n",
    "    dataset= dataset.groupby('Texto', as_index=False).first()\n",
    "    dataset = dataset.sort_values(by=['Data'])\n",
    "    dataset = dataset[['Data', 'endereco', 'Texto', 'Num_Palavras-Chave']]\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO NOTICIAS NÃO RELEVANTES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless(dataset, min_crit=1):\n",
    "\n",
    "    palavras_chave = [\n",
    "        'Agronegócio',\n",
    "        'Agricultura',\n",
    "        'Pecuária',\n",
    "        'Agroindústria',\n",
    "        'Produção Agrícola',\n",
    "        'Cultivo',\n",
    "        'Plantio',\n",
    "        'Colheita',\n",
    "        'Agropecuária',\n",
    "        'Gestão Rural',\n",
    "        'Tecnologia no Campo',\n",
    "        'Irrigação',\n",
    "        'Fertilizantes',\n",
    "        'Defensivos Agrícolas',\n",
    "        'Mercado Agrícola',\n",
    "        'Exportação Agrícola',\n",
    "        'Logística Agrícola',\n",
    "        'Sustentabilidade no Agronegócio',\n",
    "        'Agrobusiness',\n",
    "        'Cadeia Produtiva'\n",
    "    ]\n",
    "      \n",
    "    dataset['Texto'] = dataset['Texto'].astype(str)\n",
    "    \n",
    "    # Função para contar o número de palavras-chave encontradas em cada texto\n",
    "    def count_keywords(text):\n",
    "        return sum(keyword.lower() in text.lower() for keyword in palavras_chave)\n",
    "    \n",
    "    # Criar uma nova coluna com o número de palavras-chave encontradas para cada texto\n",
    "    dataset['Num_Palavras-Chave'] = dataset['Texto'].apply(count_keywords)\n",
    "    \n",
    "    # Aplicar a máscara para manter apenas os textos relevantes\n",
    "    mask = dataset['Num_Palavras-Chave'] >= min_crit\n",
    "    dataset = dataset[mask]\n",
    "    \n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO STOPWORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_br(text):\n",
    "    stop_words_pt = set(stopwords.words('portuguese'))    \n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words_pt]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def remove_stopwords_en(text):\n",
    "    stop_words_pt = set(stopwords.words('english'))    \n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words_pt]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO POTUAÇÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pontuaiton(text):\n",
    "    punctuation = set(string.punctuation)\n",
    "    for i in text:\n",
    "        if i in punctuation:\n",
    "            text = text.replace(i, \"\")\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO NÚMEROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    string = text\n",
    "    return ''.join(filter(lambda z: not z.isdigit(), string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **REMOVENDO EXPRESSOES REGULARES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_expressoes(text):\n",
    "#    return re.sub(r\"\\\\n|\\\\r|-\", \" \", text)\n",
    "\n",
    "def remove_expressoes(text):\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r'\\r', \" \", text)\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "    text = re.sub(r'\\t', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradutor(texto):\n",
    "    #time.sleep()  # Adiciona um atraso de 1 segundo entre chamadas\n",
    "    print('traduzindo')\n",
    "    return TextBlob(texto).translate(from_lang=\"pt\", to='en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRATANDO OS DADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_dados(dataset):\n",
    "    dataset = remove_useless(dataset)\n",
    "    dataset.loc[:, 'Texto'] = dataset['Texto'].map(remove_expressoes)\n",
    "    dataset['Traducao'] = dataset['Texto'].apply(tradutor)\n",
    "    \n",
    "    dataset['Texto'] = dataset['Texto'].map(lambda x:str(x).lower())\n",
    "    dataset[['Traducao', 'Texto']] = dataset[['Traducao', 'Texto']].applymap(remove_stopwords_br)\n",
    "    dataset[['Traducao', 'Texto']] = dataset[['Traducao', 'Texto']].applymap(remove_stopwords_en)\n",
    "    dataset[['Traducao', 'Texto']] = dataset[['Traducao', 'Texto']].applymap(remove_pontuaiton)\n",
    "    dataset[['Traducao', 'Texto']] = dataset[['Traducao', 'Texto']].applymap(remove_numbers)\n",
    "\n",
    "    dataset.to_excel('noticicas_tratadas_agronegocio.xlsx', index = False)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tratar_dados_temp(dataset):\n",
    "    dataset = remove_useless(dataset)\n",
    "    dataset = drop_duplicate_news(dataset)\n",
    "    \n",
    "    dataset.loc[:, 'Texto'] = dataset['Texto'].map(remove_expressoes)\n",
    "\n",
    "    print(\"Iniciando Traduções\")\n",
    "    dataset['Traducao'] = dataset['Texto'].apply(lambda x: TextBlob(x).translate(from_lang=\"pt\", to='en')).astype('str')\n",
    "\n",
    "    dataset['Texto'] = dataset['Texto'].map(lambda x:str(x).lower())\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_stopwords_br)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_stopwords_en)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_pontuaiton)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_numbers)\n",
    "\n",
    "    dataset.to_excel('noticicas_tratadas_agronegocio.xlsx', index = False)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ANÁLISE DE SENTIMENTOS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOKENIZAÇÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def token_br(dataset_noticias):\n",
    "    # Aplicar a tokenização à coluna 'texto'\n",
    "    dataset_noticias['token'] = dataset_noticias['Texto'].apply(tokenize_text)\n",
    "\n",
    "    # Replicar as linhas para cada palavra\n",
    "    dataset_token = dataset_noticias.explode('token')\n",
    "    dataset_token.drop(['Texto', 'endereco', 'Num_Palavras-Chave'], axis=1, inplace=True)\n",
    "    dataset_token.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    return dataset_token\n",
    "\n",
    "def token_en(dataset_noticias):\n",
    "    # Aplicar a tokenização à coluna 'texto'\n",
    "    dataset_noticias['token'] = dataset_noticias['Traducao'].apply(tokenize_text)\n",
    "\n",
    "    # Replicar as linhas para cada palavra\n",
    "    dataset_token = dataset_noticias.explode('token')\n",
    "    dataset_token.drop(['Texto'], axis=1)\n",
    "    dataset_token.reset_index(drop=True, inplace=True)\n",
    "    dataset_token = dataset_token[['Data', 'token']]\n",
    "\n",
    "    return dataset_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ANALISE DE SENTIMENTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_br(dataset_token):\n",
    "    Loughan_Mc = pd.read_excel(r\"C:\\Users\\eusou\\OneDrive\\Documentos\\TCC\\06. Dicionários\\Loughran_McDonald_pt.xlsx\")\n",
    "\n",
    "    Loughan_Mc = Loughan_Mc.loc[(Loughan_Mc['sentimento'] == 'positivo') | (Loughan_Mc['sentimento'] == 'negativo')]\n",
    "    \n",
    "    sentiment_analysis = None\n",
    "    sentiment_analysis = pd.merge(dataset_token, Loughan_Mc, on='token')\n",
    "    sentiment_analysis['Data'] = pd.to_datetime(sentiment_analysis['Data'], format='%d/%m/%Y')\n",
    "    sentiment_analysis = sentiment_analysis.groupby(['Data', 'sentimento']).size().unstack(fill_value=0)\n",
    "    sentiment_analysis['Sentimento'] = ((sentiment_analysis['positivo'] - sentiment_analysis['negativo']) / (sentiment_analysis['positivo'] + sentiment_analysis['negativo']))\n",
    "\n",
    "    sentiment_analysis.to_excel('Sentiment Analysis_agrongocio_br.xlsx')\n",
    "\n",
    "    return sentiment_analysis\n",
    "\n",
    "def sentiment_analysis_en(dataset_token):\n",
    "    Loughan_Mc = pd.read_excel(r\"C:\\Users\\eusou\\OneDrive\\Documentos\\TCC\\06. Dicionários\\Loughran_McDonald_en.xlsx\")\n",
    "    Loughan_Mc = Loughan_Mc.loc[(Loughan_Mc['sentiment'] == 'positive') | (Loughan_Mc['sentiment'] == 'negative')]\n",
    "    \n",
    "    sentiment_analysis = None\n",
    "    sentiment_analysis = pd.merge(dataset_token, Loughan_Mc, on='token')\n",
    "    sentiment_analysis['Data'] = pd.to_datetime(sentiment_analysis['Data'], format='%d/%m/%Y')\n",
    "    sentiment_analysis = sentiment_analysis.groupby(['Data', 'sentiment']).size().unstack(fill_value=0)\n",
    "    sentiment_analysis['sentiment'] = ((sentiment_analysis['positive'] - sentiment_analysis['negative']) / (sentiment_analysis['positive'] + sentiment_analysis['negative']))\n",
    "\n",
    "    sentiment_analysis.to_excel('Sentiment Analysis_mercado_agronegocio_en.xlsx')\n",
    "\n",
    "    return sentiment_analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SEMENALIZANDO OS SENTIMENTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanalização(sentiment_analysis):\n",
    "\n",
    "    sentiment_analysis = sentiment_analysis.resample(\"M\").mean()\n",
    "\n",
    "    return sentiment_analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PLOT TS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_analysis(sentiment_analysis, title:str):\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sentiment_analysis.index, sentiment_analysis['Sentimento'], label='Sentimento', linestyle='-')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LABORATÓRIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = initialize_g1(start='2010-05-01', source= 'g1')\n",
    "dataset_text = get_http_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APENAS UM CRITÉRIO POIS NÃO HÁ TANTAS IRREGULARIDADES NA BASE DE DADOS\n",
    "\n",
    "dataset_text = pd.read_excel('2.textos_completos_agro.xlsx')\n",
    "noticias_tratadas = tratar_dados_temp(dataset_text)\n",
    "tokens_br = token_br(noticias_tratadas)\n",
    "sentiment_analysis_br = sentiment_analysis_br(tokens_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRADUÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_dados_temp(dataset):\n",
    "    dataset = remove_useless(dataset)\n",
    "    dataset = drop_duplicate_news(dataset)\n",
    "    \n",
    "    dataset.loc[:, 'Texto'] = dataset['Texto'].map(remove_expressoes)\n",
    "\n",
    "    print(\"Iniciando Traduções\")\n",
    "    dataset['Traducao'] = dataset['Texto'].apply(lambda x: TextBlob(x).translate(from_lang=\"pt\", to='en')).astype('str')\n",
    "\n",
    "    dataset['Texto'] = dataset['Texto'].map(lambda x:str(x).lower())\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_stopwords_br)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_stopwords_en)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_pontuaiton)\n",
    "    dataset['Texto'] = dataset['Texto'].map(remove_numbers)\n",
    "\n",
    "    dataset.to_excel('noticicas_tratadas_agronegocio.xlsx', index = False)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noticias_tratadas = tratar_dados_temp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y['Texto'] = y['Texto']\n",
    "#y['Texto'] = y['Texto'].map(lambda x:Word(x).maketrans())\n",
    "y = pd.read_excel('2.textos_completos_agro.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = remove_useless(y)\n",
    "y = drop_duplicate_news(y)\n",
    "y.loc[:, 'Texto'] = y['Texto'].map(remove_expressoes)\n",
    "y.loc[:, 'Texto'] = y['Texto'].map(lambda x:str(x).replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n",
      "Texto excede os 5mil caracteres\n"
     ]
    }
   ],
   "source": [
    "def translate(texto):\n",
    "\n",
    "    tradutor = GoogleTranslator(source=\"pt\", target=\"en\")\n",
    "\n",
    "    try:\n",
    "        texto_traduzido = tradutor.translate(texto)\n",
    "        return texto_traduzido\n",
    "    \n",
    "    except:\n",
    "        print(\"Texto excede os 5mil caracteres\")\n",
    "        return \"Texto excede os 5mil caracteres\"\n",
    "\n",
    "y['Tradução'] = y['Texto'].map(translate)\n",
    "\n",
    "#12m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "376ddfc2cfde59ffc9aa391d932cec61d6ebffcf8bea694e629de6792ae67389"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
